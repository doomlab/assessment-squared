---
title             : "The title"
shorttitle        : "Title"

author: 
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Ernst-August Doelle"
    affiliation   : "1,2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  One or two sentences to put the results into a more **general context**.
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"

header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
output            : papaja::apa6_pdf
appendix          :
  - "appendix_1.Rmd"
bibliography: references.bib
---

```{r setup, include = FALSE}
# libraries
library("papaja")
library(dplyr)
library(visualizemi)
library(rio)
library(knitr)
library(ggplot2)
library(tidyr)

# references
r_refs("references.bib")
```

```{r trackdown, eval = F}
source("../../../../files/google_creds.R")
# initial set up 
# upload_file(file = "rr_manuscript.Rmd",
#             gpath = "Assessment of Assessment",
#             gfile = "rr_manuscript_trackdown")
# from google 
download_file(file = "rr_manuscript.Rmd",
            gpath = "Assessment of Assessment",
            gfile = "rr_manuscript_trackdown")
# to google
update_file(file = "PG_Manuscript_2023.Rmd",
            gfile = "grade_lean_trackdown"
            rich_text = TRUE,
            hide_code = TRUE)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(3894389)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Psychological research involves the difficult task of assessing non-observable phenomena, such as depression or meaning in life, as a measurement proxy for analyzing hypotheses. Researchers develop surveys or instruments to estimate the underlying construction of interest [@devellis2022], which is then often validated with latent variable modeling (i.e., structural equation modeling; SEM) or item response theory [IRT, @byrne2001]. Confirmatory factor analysis (CFA) is the most common choice for examining questionnaire's dimensionality, item-latent variable structure, and the overall model fit. Entire journals, such as *Assessment,* are devoted to the publication of scale development and (re)-assessment across different group populations - a necessary avenue given that development information for many scales is not reported in other journal articles [@barry2014; @weidman2017]. These manuscripts are crucial to interpretation of studies that use these measures and determining the usefulness of overall measured scores [@flake2020].

Generalized measures are designed, in theory, to provide the same assessment for different populations [@meredith1993]; however, there is growing interest in examining for differential responding across sub-populations based on potentially confounding variables. Measurement invariance (or equivalence) implies that the instrument provides the same latent variable measurement for all populations. Equality in measurement is desirable, as it allows for the same interpretation of latent variable scores across groups, as well as knowing that group differences are not attributable to differences in measurement. Non-invariance implies that individuals in separate populations interpret items differently [@liu2017; @cheung2000; @wicherts2005; @dong2020], which may affect the overall latent variable score; thus, making it difficult to know if group differences are due to population differences or measurement. Being unaware of non-invariance in measures could lead to incorrect interpretations of group differences [@vandeschoot2015], and these results could potentially explain the replication or lack-thereof for results across studies [@maassen2023]. @maassen2023 explores the reporting of measurement invariance tests across *Judgment and Decision Making*, *PLOS ONE*, and *Psychological Science* and found abysmal results: very few papers included measurement invariance tests, none of those reported tests could be reproduced, and very little measurement invariance was found across when new tests could be examined.

This study demonstrates the need for an investigation of measurement invariance within a journal that specifically targets assessments as the area of publication in *Assessment.* Given differences in cultural, experience, language skills, and more, we may not expect all measurements to show invariance across populations. Partial invariance extends multi-group testing of measurement invariance to show exactly where and how many parameters are non-invariant [@byrne1989; @meredith1993]. Understanding these items can lead to further investigation into group differences, new interpretation guidelines, or scale improvement to eliminate those differences. Measurement invariance testing suffers from the same black-and-white judgment criteria found in traditional null hypothesis testing and *p*-values with cutoff criteria and rules of thumb [@marsh2004; @putnick2016]. $d_{MACS}$ was developed as an effect size for measurement invariance for group differences in observed variables which is affected by both factor loadings and item intercepts [@nye2011]. `visualizemi` is a new *R* package that calculates the "replication" rate of the overall model steps in measurement invariance (as compared to randomized data), as well as the effect sizes for individual parameters, separating loadings, intercepts, residuals, and so on.

As shown by @maassen2023, measurement invariance may be a hard standard to meet. At the moment, it is difficult to know what effect sizes one may expect to find for measurement invariance and what may be a level of measurement invariance to worry about (i.e., moving away from invariant or not decisions). Researchers may be able to define a smallest effect size of interest in measurement invariance [@anvari2021; @lakens2018]. In this registered report, we propose to examine the studies published within *Assessment* that report measurement invariance. We will create a database of studies that report measurement invariance and code these articles for the type of groups tested, steps of measurement invariance performed, and results obtained. This searchable database can be useful for further meta-research on measurement invariance or simple lookup for individuals searching for measurement instruments. Next, we will reproduce measurement invariance tests for publications with data and calculate the effect sizes for model and parameter level invariance. This information will be provided in the database to allow researchers to gauge what they may expect if they use a questionnaire or wish to replicate/extend previous work. We will provide the overall summary of effect sizes within measurement invariance tests and comment on the distributions of effect sizes found within the literature. We will end by providing researchers with suggestions for ways to determine their smallest effect size of interest for pre-registration or practical assessment.

# Proposed Method

## Database Curation

```{r figure-process, fig.cap="A flow chart of potential exclusions to create the database of measurement invariance."}
include_graphics("pics/data_curation.png")
```

Using *Assessment*'s online search feature, we will search for **measurement invariance** allowing for either term to be present in the manuscript for inclusion in the first round of papers. As of February 2024, this search returns over 600 articles from 1994 to 2024 publications. The data will then be filtered to only include research articles under the article type filter present on the journal website. These citations will be exported to the Zotero group created for this project found at: <https://www.zotero.org/groups/5407184/measurement_invariance_assessment>. Figure \@ref(fig:figure-process) displays the filtering and coding process to create the measurement invariance database.

Each article will then be coded for inclusion in the measurement invariance database and for potential further analysis. The coding scale is included in Appendix A. Coders will be first asked to determine if the article includes measurement invariance, and they will be instructed to include articles that use structural equation modeling or item response theory or leave notes if they are unsure. For articles marked yes or unsure, they will then enter if the article uses structural equation modeling or item response theory, as well as a potential other category. Last, they will indicate if the article includes real data. At this point in the survey, studies that have been marked as not measurement variance, item response theory, or simulation/theoretical articles will be excluded.

The next portion of the coding survey includes information about the measurement invariance test(s) in the manuscript. Each measurement test will be coded separately. Coders will include the name and citation of the scale assessed, what groups are compared in the measurement invariance test, and the steps performed in the measurement invariance assessment. Once these steps are selected, coders will order them based on the manuscript, list the type of invariance claimed, and list the fit index used for determination of invariance, along with the rule (i.e., CFI, $\Delta$CFI \> .01). Finally, they will determine if the data (covariance/correlation matrices or raw data) is avaliable. If so, they will upload it to a private repository for the analysis portion of the study. Coders will be recruited from author networks specifically for individuals with experience in structural equation modeling. They will be given a video example of the coding procedure to watch before beginning the process.

```{r}
DF <- import("../data/Article_Coding_Assessment_pilot.csv")

DF2 <- DF %>% 
  select(invariance_yes, cfa_irt, real_data) %>% 
  filter(!duplicated(DF %>% select(doi_link)))

yes_mi <- round(table(DF2$invariance_yes, useNA = "ifany") / nrow(DF2) * 100, 1)
yes_sem <- round(table(DF2$cfa_irt[DF2$cfa_irt != ""], useNA = "ifany") / sum(table(DF2$cfa_irt[DF2$cfa_irt != ""], useNA = "ifany") ) * 100, 1)
yes_data <- round(table(DF2$real_data[DF2$real_data != ""], useNA = "ifany") / sum(table(DF2$real_data[DF2$real_data != ""], useNA = "ifany") ) * 100, 1)

DF3 <- DF %>% 
  filter(scale_name != "")

yes_data2 <- round(table(DF3$data, useNA = "ifany") / nrow(DF3) * 100, 1)

est_sem <- round(600 * yes_sem["Structural Equation Modeling"] / 100, 0)
est_data <- round(est_sem * yes_data["Yes"] / 100, 0)
est_data2 <- round(est_data * yes_data2["Yes"] / 100, 0)
```

A pilot test of this coding procedure was completed with the lead author's structural equation modeling course. Each coder was assigned a specific issue of Volume 30 and examined all articles within that issue. Approximately `r yes_mi["Yes"]` percent of articles within those issues included measurement invariance (*n* = `r nrow(DF2)` unique articles coded). Of those articles, `r yes_sem["Structural Equation Modeling"]` % used structural equation modeling, and of those articles, `r yes_data["Yes"]` percent used participant data. `r nrow(DF3)` measurement invariance tests were coded, and approximately `r yes_data2["Yes"]` percent included data for testing.

If the search results online return articles that are more closely aligned with measurement invariance (rather than examining all articles as we did in the pilot study), we might expect that a smaller proportion will be excluded for only discussing measurement invariance. If the other percentages are approximate, then we might expect that `r est_sem` articles would use structural equation modeling, `r est_data` would include participant data, and `r est_data2` may have avaliable data for examination. Given the recent trends in transparency and openness, this number is likely an overestimate as the pilot included only new articles.

The final coding of articles and their measurement invariance tests will be presented as a database of measurement invariance results. These results can be re-used in other meta-analytic studies. We will present the following summary statistics:

-   Prevalence of Measurement Invariance
    -   Total number of invariance related articles
    -   Split of articles into structural equation modeling, item response theory, and other analyses types
    -   Number of articles that include participant data

Each of these statistics will be calculated on the unique articles coded.

-   Measurement Invariance Test Statistics
    -   Commonly reported scales (if any patterns emerge)
    -   Frequency of number of groups compared
    -   Commonly occurring group comparisons: we will code this variable into overall category such as age, gender/sex, language, etc.
    -   Frequency of each type of measurement invariance assessment (i.e., number of equal form, equal item intercepts, etc.)
    -   Commonly used fit statistics and rules of thumb for invariance testing
    -   Commonly reported level of invariance
    -   Frequency of data inclusion for reproducibility

## Data Analysis

Each measurement invariance test that included data will then be coded using the template in Appendix 2. Using the `visualizemi` package [CITE], coders will program a multigroup confirmatory factor analysis using the steps outlined from the research article and the provided data. Each model will first be examined for convergence across the same measurement invariance steps from the published articles. Models that do not converge will be noted, and no more coding will be performed. Given the results from the article, each model will then be tested for replication effect sizes rates at the model and parameter level. For example, an article that claimed measurement invariance for equal form, loadings, and intercepts will be tested with these same steps to determine the effect size of potential replication for each of those steps.

The `visualizemi` package calculates the effect size of potential replication by bootstrapping the original data with replacement from the study and compares these results to a the same bootstrapped dataset that has had the group labels randomized. The effect size is normalized based on the range of *h* (effect size for two proportions, similar to Cohen's *d*) to create a score of -1 to 1. A positive non-measurement invariance effect size means that the bootstrapped data has more non-invariance than the randomized scores. A negative effect size would indicate that the bootstrapped data has less non-invariance than the randomized data. A score of zero indicates that the bootstrapped and randomized data have the same likelihood of (non)-invariance. The logic of desiring a zero or negative effect size is that it implies that the groups are equivalent, and therefore, the randomization does not affect the results because groups perform the same on the questionnaire. Because this effect size is based on proportions, one could flip the sign for the effect size for measurement invariance, but here we focus on the amount of non-invariance.

The potential "partial" invariance for each parameter at the final step of the model testing will then be examined. In our example, the intercept level would be examined for parameter level effect sizes, as it was the final step of their invariant model. If an article reported partial invariance, the step with partial invariance will be examined at the parameter level. If they used multiple partial invariance steps, only the final stage will be examined. The same effect size can be calculated for each parameter in the partial invariance test, as well as the difference in group parameter estimates (i.e., *d* for intercept group 1 versus intercept group 2).

Each coding will be programmed by one coder and checked by another coder using a small number of bootstrap simulations. After both agree that the coding matches the article data, the model and parameter level simulations will be run over 1000 simulations on a high performance computing server to ensure consistency in versions of *R* and packages. The exact versions will be reported for reproducibility. The results will be exported in both an HTML format and Rdata for further use. These files will be available on our repository at <https://github.com/doomlab/assessment-squared>.

### Model Level Results

At the model level, the bootstrapping function returns an effect size `h_nmi_p` which represents the *h* effect size statistic for measurement invariance normalized to a proportion of the possible effect size ranged. We will compile the results based on step across measurement invariance tests, and we will report the following (simulated data below):

-   Mean, standard deviation, sample size for each step
-   25, 50, 75% quantile values
-   Visualization of the distribution of effects - for example, Figure \@ref(fig:figure-violin)

```{r figure-violin, fig.cap="A visualization of the effect sizes for measurement invariance at the model level."}
Intercepts <- rnorm(n = 100, mean = -.2, sd = .10)
Loadings <- rnorm(n = 100, mean = -.4, sd = .30)
Residuals <- rnorm(n = 100, mean = -.12, sd = .15)

DF_model <- data.frame(Intercepts, Loadings, Residuals) %>% 
  pivot_longer(cols = everything(), 
               names_to = "step", 
               values_to = "h")

ggplot(DF_model, aes(step, h)) + 
  theme_classic() + 
  geom_violin() + 
  geom_boxplot(width = .2) + 
  xlab("Measurement Invariance Step") + 
  ylab("h Measurement Invariance")
```

### Parameter Level Results

For the parameter level results, the bootstrapping function returns similar information for each of the parameters in the final step of the model. For example, if a manuscript suggests that the model was fully invariant at the residuals level, each residual effect would be tested to determine effect sizes of replication and group differences. First, we will extract the `h_nmi_p` effects for each parameter to determine the distribution, statistics, and visualizations for each type of parameter (loadings, residuals, intercepts, etc.). We will present:

-   Mean, standard deviation, sample size for each type of parameter
-   25, 50, 75% quantile values
-   Visualization of the distribution of effects

Next, the function additionally returns effect sizes for group differences on each parameter, $d_s$. This value is calculated as the average mean difference of the parameter estimates for each group divided by their pooled standard error across bootstraps. $d_s$ is calculated for both the bootstrapped data and the randomized data. We will calculated a "normalized" effect size for each parameter by subtracting $d_s$ for the bootstrapped data minus the $d_s$ for the randomized data. Because group order is arbitrary in our analysis, we will take the absolute value of the effect size for final reporting. From this dataset, we will report:

-   Mean, standard deviation, sample size for each type of parameter
-   25, 50, 75% quantile values
-   Visualization of the distribution of effects

## Results Interpretation

Our study is exploratory to determine the landscape of measurement invariance effect size using the premier journal outlet for such publications in clinical psychology. We make no predictions on the direction or size of the results. Instead, we will present the database for future reuse and examine the results for any consistent patterns or findings. By understanding the range of published values[^1], researchers can use these results to gauge their study results against. We will comment on potential ideas for estimating the smallest effect of interest for measurement invariance.

[^1]: The advantage of studying measurement invariance in this scenario is that both invariant and non-invariant models are typically publishable. While publication bias likely still exists, it may be less than traditional effect size meta-analysis studies.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
